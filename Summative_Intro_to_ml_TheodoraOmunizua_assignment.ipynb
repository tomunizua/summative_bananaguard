{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomunizua/summative_bananaguard/blob/main/Summative_Intro_to_ml_TheodoraOmunizua_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization Techniques in Machine Learning\n",
        "\n",
        "Objective: This assignment aims to explore implementation or Machine Learning Models with regularization, optimization and Error analysis  techniques used in machine learning to improve models' performance, convergence speed, and efficiency..\n",
        "\n",
        "A Notebook detailing the following\n",
        "\n",
        "* Project name\n",
        "* Clear out puts from cells\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "1. Acquire a dataset suitable for ML tasks as per your proposal.\n",
        "2. Implement a simple machine learning model based on neural networks on the chosen dataset without any defined optimization techniques. (Check instructions)\n",
        "3. Implement and compare the model's performance after applying 3 to 4 disntict combinations regularization and optimization techniques.\n",
        "4. Discuss the results on the README file.\n",
        "5. Make predictions using test data\n",
        "7. Implement error analysis techniques and ensure there is: F1-Score, Recall, Precision, RUC a confusion matrix using plotting libraries (not verbose)\n",
        "\n",
        "Submit notebook to github repo\n",
        "\n"
      ],
      "metadata": {
        "id": "ujuLMxSvoXbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Case Study and Implementation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o8VW_IzbI3od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Necessary Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "# For PyTorch and YOLOv5\n",
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "#!pip install yolov5\n",
        "from yolov5 import YOLOv5\n"
      ],
      "metadata": {
        "id": "wGCnpzs9M4Fd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Dataset\n",
        "> ***Brief Description:***\n",
        "State the Problem and A short Description of the data\n"
      ],
      "metadata": {
        "id": "M1FN7bFeIxfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"D84UstygUMTvzYCf2Rhd\")\n",
        "project = rf.workspace(\"projectia-khxte\").project(\"segment-tyeq6\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QwSZ-kEuxUA",
        "outputId": "ab6fef3f-1a85-4e28-ee78-89e716b8a39a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Segment-1 to yolov5pytorch:: 100%|██████████| 107160/107160 [00:02<00:00, 44157.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Segment-1 in yolov5pytorch:: 100%|██████████| 2812/2812 [00:00<00:00, 3361.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: Load Data (Seprate into: Train, Validation and test sets)\n",
        "\n",
        "train_image_dir = '/content/Segment-1/train/images'\n",
        "train_label_dir = '/content/Segment-1/train/labels'\n",
        "\n",
        "valid_image_dir = '/content/Segment-1/valid/images'\n",
        "valid_label_dir = '/content/Segment-1/valid/labels'\n",
        "\n",
        "test_image_dir = '/content/Segment-1/test/images'\n",
        "test_label_dir = '/content/Segment-1/test/labels'\n",
        "\n",
        "def get_file_paths(image_dir, label_dir):\n",
        "    image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.jpg')]\n",
        "    label_paths = [os.path.join(label_dir, fname) for fname in os.listdir(label_dir) if fname.endswith('.txt')]\n",
        "    return image_paths, label_paths\n",
        "\n",
        "train_images, train_labels = get_file_paths(train_image_dir, train_label_dir)\n",
        "valid_images, valid_labels = get_file_paths(valid_image_dir, valid_label_dir)\n",
        "test_images, test_labels = get_file_paths(test_image_dir, test_label_dir)\n",
        "\n",
        "def preprocess_labels(label_path):\n",
        "    with open(label_path, 'r') as file:\n",
        "        labels_str = file.read().split()\n",
        "    # Handle potential empty label files:\n",
        "    if not labels_str:\n",
        "        return np.array([], dtype=np.float32)  # Or a suitable default value\n",
        "    try:\n",
        "      labels = np.array([float(x) for x in labels_str], dtype=np.float32)\n",
        "    except ValueError as e:\n",
        "      print(f\"Error converting labels in {label_path}: {e}\")\n",
        "      print(f\"Problematic string: {labels_str}\")\n",
        "      return np.array([], dtype=np.float32) #Or a suitable default value\n",
        "    return labels\n",
        "\n",
        "\n",
        "train_labels = [preprocess_labels(path) for path in train_labels]\n",
        "valid_labels = [preprocess_labels(path) for path in valid_labels]\n",
        "test_labels = [preprocess_labels(path) for path in test_labels]\n",
        "\n",
        "def load_image(image_path):\n",
        "    image_string = tf.io.read_file(image_path)\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image_resized = tf.image.resize(image_decoded, [128, 128])\n",
        "    return image_resized\n",
        "\n",
        "def create_dataset(images, labels):\n",
        "    ragged_labels = tf.ragged.constant(labels)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({\"image\": images, \"label\": ragged_labels})\n",
        "\n",
        "    def process_path(data):\n",
        "        image_path = data[\"image\"]\n",
        "        label = data[\"label\"]\n",
        "        image = load_image(image_path)\n",
        "        return {\"image\": image, \"label\": label}\n",
        "\n",
        "    dataset = dataset.map(process_path)\n",
        "\n",
        "    dataset = dataset.padded_batch(\n",
        "        batch_size=32,\n",
        "        padded_shapes={\"image\": [128, 128, 3], \"label\": [None]},\n",
        "        padding_values={\"image\": tf.constant(0.0, dtype=tf.float32), \"label\": tf.constant(-1.0, dtype=tf.float32)}\n",
        "    )\n",
        "\n",
        "    dataset = dataset.shuffle(buffer_size=100)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = create_dataset(train_images, train_labels)\n",
        "valid_dataset = create_dataset(valid_images, valid_labels)\n",
        "test_dataset = create_dataset(test_images, test_labels)\n"
      ],
      "metadata": {
        "id": "nas-T7xwPIso"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECTION 1: Model Architecture:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "TODO: Insert an image with the Model architecture here.Replace the image Below\n",
        "```\n",
        "> <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*v1ohAG82xmU6WGsG2hoE8g.png\" alt=\"?\" style=\"width:25px\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ve4AiQmGMzIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task: Define a function that creates models without and With specified Optimization techniques\n"
      ],
      "metadata": {
        "id": "QR4BNYoUMzMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Define a simple neural network model\n",
        "def define_noneoptimizer_model():\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=(128, 128, 3)))  # Adjust input shape accordingly\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model without specifying any optimizer\n",
        "  model.compile(optimizer=None, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training dataset\n",
        "  history = model.fit(train_dataset, validation_data=valid_dataset, epochs=10)\n",
        "\n",
        "# Define and train the simple model\n",
        "  noneoptimizer_model = define_noneoptimizer_model()\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "  results = noneoptimizer_model.evaluate(test_dataset)\n",
        "  print(f\"Test Accuracy: {results[1]*100:.2f}%\")\n",
        "\n",
        "# def define_model(optimization: string, regularization_datatype, early_stopping: bool, dropout: float, learning_rate: float):\n",
        "#   model= None\n",
        "#   model.add(None)\n",
        "#   #TO DO: Add more layers as per architecture\n",
        "#   model.add(None) # Last Layer\n",
        "#   model.compile(optimizer = optimizerNone)\n",
        "#   model.fit(None)\n",
        "#   return model"
      ],
      "metadata": {
        "id": "gGtPmYb_SDHy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Print out the Final Model Accuracy and plot the Loss curve"
      ],
      "metadata": {
        "id": "KJ9OXp1TSaXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_curve_plot(history):\n",
        "  epochs = range(1, len(history.history['loss']) + 1)\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "fkihQBsaUxGh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 2: Optimization and Regularization Combinations\n",
        "At this point you should now create models that combine various optimization techniques\n",
        "As done before make sure to plot out the loss curve and the accuracy and loss in verbose"
      ],
      "metadata": {
        "id": "9hc6k2taT0_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n",
        "model_2 = define_model('Adam', None)\n",
        "loss_curve_plot(model_2):\n",
        "#print out confusion matrix and error analysis metrics after the cell"
      ],
      "metadata": {
        "id": "nr6HuVEtXvoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n",
        "model_3 = define_model('RMSPop',None)\n",
        "loss_curve_plot(model_3):\n",
        "#print out confusion matrix and error analysis metrics after the cell"
      ],
      "metadata": {
        "id": "UygzqjB-Xvgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n",
        "model_4 = define_model(None)\n",
        "loss_curve_plot(model_4):\n",
        "#print out confusion matrix and error analysis metrics after the cell"
      ],
      "metadata": {
        "id": "9NEtnjqxXvXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task: Make Predictions using the best saved model\n"
      ],
      "metadata": {
        "id": "3aZLEriYOXI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a confusion Matrix and F1 score for both Models. Ensure outputs for the cells are visible"
      ],
      "metadata": {
        "id": "LnBLwqTJX3p0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, Make predictions using the best model. By the time you get to this cell you may realise at some point you needed to save the model so that you cal load it later"
      ],
      "metadata": {
        "id": "bO79SOsZYG-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model_path, X):\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(None)\n",
        "    # Make predictions\n",
        "    predictions = None\n",
        "    # Convert probabilities to binary labels (0 or 1)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "#Modify the code appropriately"
      ],
      "metadata": {
        "id": "Nqqe2PasUIAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = None\n",
        "make_predictions(None)"
      ],
      "metadata": {
        "id": "O_jwbvaAUMj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations!!\n"
      ],
      "metadata": {
        "id": "rfTHk2nZMzTH"
      }
    }
  ]
}